# Impl√©mentation OpenRouter, LiteLLM et E2B Desktop dans notre app

Objectif: fournir aux devs un guide concis et op√©rationnel pour int√©grer OpenRouter, LiteLLM Proxy et E2B Desktop Sandbox, avec outils (tool calling), streaming SSE et s√©curit√©.

---

## 1) Pr√©-requis et en-t√™tes

* API base

  * OpenRouter: `https://openrouter.ai/api/v1` (compat. OpenAI) ([OpenRouter][1])
  * LiteLLM Proxy: endpoint OpenAI-compatible expos√© localement par votre passerelle. ([docs.litellm.ai][2])
* Auth

  * `Authorization: Bearer <OPENROUTER_API_KEY>` pour OpenRouter. ([OpenRouter][3])
* Attribution facultative c√¥t√© **serveur** ou **navigateur** si n√©cessaire:

  * `HTTP-Referer: <APP_URL>`
  * `X-Title: <APP_NAME>` ([OpenRouter][4])
* Outils (tools / function calling): sch√©ma OpenAI-compatible standardis√© par OpenRouter. ([OpenRouter][5])

---

## 2) OpenRouter ‚Äî appels de base

### 2.1 curl

```bash
# Liste des mod√®les
curl -s https://openrouter.ai/api/v1/models \
  -H "Authorization: Bearer $OPENROUTER_API_KEY"

# Chat completions (non-stream)
curl -s https://openrouter.ai/api/v1/chat/completions \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openrouter/auto",
    "messages": [
      {"role": "user", "content": "Hello"}
    ]
  }'
```

Format conforme √† l‚ÄôAPI OpenRouter. ([OpenRouter][6])

### 2.2 TypeScript (server-side)

```ts
// app/api/chat/route.ts (Next.js, runtime Node)
export const runtime = 'nodejs';

export async function POST(req: Request) {
  const { messages, model = 'openrouter/auto', stream = true } = await req.json();

  const r = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY!}`,
      'Content-Type': 'application/json',
      ...(process.env.APP_URL ? { 'HTTP-Referer': process.env.APP_URL } : {}),
      ...(process.env.APP_NAME ? { 'X-Title': process.env.APP_NAME } : {}),
      ...(stream ? { 'Accept': 'text/event-stream' } : {})
    },
    body: JSON.stringify({ model, messages, stream })
  });

  if (!stream) return new Response(await r.text(), { status: r.status });

  return new Response(r.body, { headers: { 'Content-Type': 'text/event-stream' } });
}
```

OpenRouter est OpenAI-compatible et supporte le streaming SSE. ([OpenRouter][1])

---

## 3) Tool calling (fonctions) avec OpenRouter

### 3.1 Sch√©ma d‚Äôoutils

```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "open_url_in_sandbox",
        "description": "Open a URL in the active E2B desktop sandbox.",
        "parameters": {
          "type": "object",
          "properties": {
            "url": { "type": "string" }
          },
          "required": ["url"]
        }
      }
    }
  ],
  "tool_choice": "auto"
}
```

OpenRouter standardise l‚Äôinterface de tool calling sur le format OpenAI. V√©rifiez par mod√®le le support du param√®tre `tools`. ([OpenRouter][5])

### 3.2 Boucle d‚Äôex√©cution c√¥t√© serveur

```ts
type ToolCall = {
  id: string;
  type: 'function';
  function: { name: string; arguments: string };
};

async function callOpenRouter(messages: any[], tools: any[]) {
  const res = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY!}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'openrouter/auto', messages, tools, tool_choice: 'auto'
    })
  });
  const json = await res.json();
  return json;
}

export async function POST(req: Request) {
  const { messages } = await req.json();
  const tools = [/* cf. sch√©ma ci-dessus */];

  // 1) Appel mod√®le
  const first = await callOpenRouter(messages, tools);

  // 2) Si tool_calls, ex√©cuter c√¥t√© serveur puis renvoyer le r√©sultat au mod√®le
  const calls = first.choices?.[0]?.message?.tool_calls as ToolCall[] | undefined;

  if (calls?.length) {
    const toolOutputs = await Promise.all(calls.map(async (c) => {
      const args = JSON.parse(c.function.arguments || '{}');
      switch (c.function.name) {
        case 'open_url_in_sandbox':
          // √† relier √† E2B (section 5)
          const result = await openUrlInE2B(args.url);
          return { tool_call_id: c.id, role: 'tool', content: JSON.stringify(result) };
        default:
          return { tool_call_id: c.id, role: 'tool', content: JSON.stringify({ error: 'Unknown tool' }) };
      }
    }));

    // 3) Re-appel LLM avec outputs des tools
    const second = await callOpenRouter(
      [
        ...messages,
        first.choices[0].message,
        ...toolOutputs
      ],
      tools
    );
    return Response.json(second);
  }

  return Response.json(first);
}
```

M√©canique: le LLM **propose** un outil. Le serveur ex√©cute l‚Äôoutil, renvoie la sortie via un message `role: "tool"` adoss√© √† `tool_call_id`, puis relance le mod√®le pour formater la r√©ponse finale. ([OpenRouter][5])

---

## 4) Gestion d‚Äôerreurs et s√©curit√© OpenRouter

* 401 `User not found` ‚Üí cl√© absente/mal transmise: v√©rifier en headers serveur, jamais c√¥t√© client. ([OpenRouter][1])
* Utiliser `HTTP-Referer`/`X-Title` uniquement si appel c√¥t√© navigateur ou attribution souhait√©e. ([OpenRouter][4])
* V√©rifier la prise en charge de `tools` par mod√®le (`supported_parameters`). ([OpenRouter][7])

---

## 5) E2B Desktop Sandbox ‚Äî cr√©ation et interaction

### 5.1 Cr√©ation et streaming VNC

```ts
import { Sandbox } from '@e2b/desktop';

export const SANDBOX_TIMEOUT_MS = 5 * 60_000;

export async function createDesktop(resolution: [number, number]) {
  const desktop = await Sandbox.create({ resolution, dpi: 96, timeoutMs: SANDBOX_TIMEOUT_MS });
  await desktop.stream.start();
  const vncUrl = desktop.stream.getUrl(); // partager au client pour affichage
  desktop.setTimeout(SANDBOX_TIMEOUT_MS);
  return { desktop, vncUrl };
}
```

API `Sandbox.create`, `stream.start()`, `stream.getUrl()` et SDK Desktop document√©s par E2B. ([e2b.dev][8])

### 5.2 Patron d‚Äôoutil reli√© √† E2B

```ts
let activeDesktop: Sandbox | undefined;

async function ensureDesktop() {
  if (!activeDesktop) {
    const { desktop } = await createDesktop([1366, 768]);
    activeDesktop = desktop;
  }
  return activeDesktop!;
}

export async function openUrlInE2B(url: string) {
  const desktop = await ensureDesktop();
  // Exemple: ouvrir un navigateur via lanceur / raccourci pr√©install√©.
  // Selon l'image Desktop, utilisez un script ou un utilitaire d√©j√† pr√©sent.
  // Ici on publie surtout l'URL VNC au client, l'ouverture du site peut √™tre
  // pilot√©e par votre couche d'automatisation (clic/keyboard).
  return { ok: true, vncUrl: desktop.stream.getUrl(), opened: url };
}
```

Notes:

* E2B Desktop fournit un **ordinateur isol√©** pr√™t pour ¬´ Computer Use ¬ª. Le SDK JS/Python sert √† d√©marrer/contr√¥ler des sandboxes. Int√©grez vos primitives d‚Äôautomatisation (clic, clavier) selon votre image de base. ([e2b.dev][9], [GitHub][10])
* Latence et diffusion: E2B √©volue vers VNC faible latence. Tenez compte des limites de flux et du *single client* selon l‚Äôimpl√©mentation. ([e2b.dev][11])
* Nettoyage: ajustez `timeoutMs`, d√©truisez les sandboxes inactifs.

---

## 6) LiteLLM Proxy ‚Äî passerelle OpenAI-compatible

### 6.1 D√©marrage

* LiteLLM fournit un **gateway** OpenAI-compatible qui unifie de multiples providers. ([docs.litellm.ai][2])
* Lancez le proxy (cf. docs ¬´ Proxy Server ¬ª) et configurez les cl√©s providers dans l‚Äôenv/proxy. ([docs.litellm.ai][12])

### 6.2 Appels via le proxy

```bash
# Exemple curl vers votre proxy local
curl -s http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer $LITELLM_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",  // ou `litellm_proxy/your-model-name` selon config
    "messages": [{"role": "user", "content": "Hello from LiteLLM"}],
    "tools": [],
    "tool_choice": "auto"
  }'
```

```ts
// TS: pointer OPENAI_BASE_URL vers LiteLLM Proxy
const r = await fetch('http://localhost:4000/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${process.env.LITELLM_API_KEY!}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({ model: 'gpt-4o-mini', messages, tools })
});
```

Le proxy parle le **m√™me** sch√©ma OpenAI, y compris tool calling et streaming, et peut router des providers h√©t√©rog√®nes. ([docs.litellm.ai][2])

---

## 7) Streaming SSE c√¥t√© client

```ts
// Client: lire un flux SSE renvoy√© par /api/chat
const resp = await fetch('/api/chat', { method: 'POST', body: JSON.stringify({ messages, stream: true }) });
const reader = resp.body!.getReader();
const decoder = new TextDecoder();

let buffer = '';
while (true) {
  const { value, done } = await reader.read();
  if (done) break;
  buffer += decoder.decode(value, { stream: true });

  const events = buffer.split('\n\n');
  buffer = events.pop() || '';

  for (const evt of events) {
    if (!evt.startsWith('data:')) continue;
    const json = JSON.parse(evt.slice(5));
    // Traiter `role`, `content`, `tool_calls`, etc.
  }
}
```

Format SSE standard support√© par OpenRouter quand `stream: true`. ([OpenRouter][6])

---

## 8) Mod√®les, capacit√©s et compatibilit√©

* V√©rifiez la prise en charge de `tools`, `tool_choice`, `json_mode`, etc., par mod√®le via `supported_parameters`. ([OpenRouter][7])
* OpenRouter expose aussi des mod√®les OpenAI via interface compatible. ([OpenRouter][13])

---

## 9) S√©curit√©, tests, debugging

* Ne jamais exposer `OPENROUTER_API_KEY` c√¥t√© client.
* Test hors app:

  * `GET /models` doit renvoyer `200`. ([OpenRouter][1])
* Erreurs usuelles:

  * `401` ‚Üí cl√© absente/r√©voqu√©e ou non transmise (header manquant). ([OpenRouter][1])
* Attribution: ajoutez `HTTP-Referer` et `X-Title` si vous voulez suivre votre app dans le ranking OpenRouter. ([OpenRouter][4])

---

## 10) Exemple complet ‚Äî cha√Æne LLM ‚Üí Tool ‚Üí E2B ‚Üí LLM

```ts
// /api/agent/route.ts
export const runtime = 'nodejs';

const tools = [{
  type: 'function',
  function: {
    name: 'open_url_in_sandbox',
    description: 'Open a URL in the active E2B desktop and return VNC URL.',
    parameters: { type: 'object', properties: { url: { type: 'string' }}, required: ['url'] }
  }
}];

export async function POST(req: Request) {
  const { messages } = await req.json();

  // 1) Premier appel ‚Äî laisser le mod√®le d√©cider d‚Äôappeler l‚Äôoutil
  let r = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY!}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ model: 'openrouter/auto', messages, tools })
  });
  let j = await r.json();

  const calls = j.choices?.[0]?.message?.tool_calls ?? [];
  if (calls.length) {
    // 2) Ex√©cuter les tools c√¥t√© serveur
    const toolMsgs = [];
    for (const c of calls) {
      const args = JSON.parse(c.function.arguments || '{}');
      if (c.function.name === 'open_url_in_sandbox') {
        const out = await openUrlInE2B(args.url);
        toolMsgs.push({ role: 'tool', tool_call_id: c.id, content: JSON.stringify(out) });
      }
    }

    // 3) Second appel ‚Äî formatter la r√©ponse finale avec les sorties des tools
    r = await fetch('https://openrouter.ai/api/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENROUTER_API_KEY!}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'openrouter/auto',
        messages: [...messages, j.choices[0].message, ...toolMsgs]
      })
    });
    j = await r.json();
  }

  return Response.json(j);
}
```

Processus fid√®le au mod√®le standardis√© par OpenRouter pour tool calling. ([OpenRouter][5])

---

## 11) Check-list d‚Äôint√©gration

* [ ] Variables d‚Äôenv: `OPENROUTER_API_KEY`, `APP_URL`, `APP_NAME`, `LITELLM_API_KEY`. ([OpenRouter][3], [docs.litellm.ai][2])
* [ ] Endpoint serveur Node (pas Edge) pour cacher la cl√©.
* [ ] SSE activ√© (`stream: true`) si n√©cessaire. ([OpenRouter][6])
* [ ] Outils d√©clar√©s (`tools`) et boucle tool-execution. ([OpenRouter][5])
* [ ] E2B Desktop: cr√©ation sandbox, `stream.start()`, `getUrl()`, timeouts. ([e2b.dev][8])
* [ ] LiteLLM Proxy d√©marr√© si vous voulez unifier des providers. ([docs.litellm.ai][2])

---

## R√©f√©rences

* OpenRouter API overview, chat completions, quickstart, tool calling, models params, attribution. ([OpenRouter][1])
* E2B Desktop SDK et docs g√©n√©rales. ([e2b.dev][8])
* LiteLLM Proxy / OpenAI-compatible. ([docs.litellm.ai][2])

---

Fin.

[1]: https://openrouter.ai/docs/api-reference/overview?utm_source=chatgpt.com "OpenRouter API Reference | Complete API Documentation"
[2]: https://docs.litellm.ai/docs/providers/litellm_proxy?utm_source=chatgpt.com "LiteLLM Proxy (LLM Gateway)"
[3]: https://openrouter.ai/docs/quickstart?utm_source=chatgpt.com "OpenRouter Quickstart Guide | Developer Documentation"
[4]: https://openrouter.ai/docs/app-attribution?utm_source=chatgpt.com "App Attribution | OpenRouter Documentation"
[5]: https://openrouter.ai/docs/features/tool-calling?utm_source=chatgpt.com "Tool & Function Calling | Use Tools with OpenRouter"
[6]: https://openrouter.ai/docs/api-reference/chat-completion?utm_source=chatgpt.com "Chat completion | OpenRouter | Documentation"
[7]: https://openrouter.ai/docs/models?utm_source=chatgpt.com "Access 400+ AI Models Through One API - OpenRouter"
[8]: https://e2b.dev/docs/sdk-reference/desktop-js-sdk/v1.1.1/sandbox?utm_source=chatgpt.com "E2B - Code Interpreting for AI apps"
[9]: https://e2b.dev/docs?utm_source=chatgpt.com "E2B - Code Interpreting for AI apps"
[10]: https://github.com/e2b-dev/desktop?utm_source=chatgpt.com "e2b-dev/desktop: E2B Desktop Sandbox for LLMs. ..."
[11]: https://e2b.dev/blog/how-i-taught-an-ai-to-use-a-computer?utm_source=chatgpt.com "How I taught an AI to use a computer"
[12]: https://docs.litellm.ai/docs/proxy_server?utm_source=chatgpt.com "[OLD PROXY üëâ NEW proxy here] Local LiteLLM Proxy Server"
[13]: https://openrouter.ai/provider/openai?utm_source=chatgpt.com "OpenAI - OpenRouter"
